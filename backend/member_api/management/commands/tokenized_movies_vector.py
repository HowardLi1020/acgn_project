from django.core.management.base import BaseCommand
import pandas as pd
import faiss
import pickle
import numpy as np
from sentence_transformers import SentenceTransformer
from django.conf import settings
import os

class Command(BaseCommand):
    help = 'Import Excel files and create FAISS index'

    def __init__(self):
        super().__init__()
        # æ¨¡å‹åç¨±èˆ‡æª”æ¡ˆè·¯å¾‘
        self.model_name = 'BAAI/bge-m3'
        # self.model_name = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'
        self.EXCEL_FILE = os.path.join(settings.BASE_DIR, 'member_api/original_data/data_movies_tokenized.xlsx')
        self.VECTOR_INDEX_PATH = os.path.join(settings.BASE_DIR, 'member_api/vector_data/tokenized_movies_vector.index')
        self.IDS_PATH = os.path.join(settings.BASE_DIR, 'member_api/vector_data/tokenized_movies_ids.pkl')

    def read_tokenized_excel(self, file_path):
        """è®€å–å·²ç¶“äººå·¥æª¢æŸ¥çš„æ–·è©çµæœ"""
        self.stdout.write(f"ğŸ“¥ å¾ {file_path} è®€å–å·²æ–·è©è³‡æ–™...")
        df = pd.read_excel(file_path)

        if 'movie_title' not in df.columns or 'tokenized_corpus' not in df.columns:
            raise ValueError("âŒ Excel å¿…é ˆåŒ…å« 'movie_title' èˆ‡ 'tokenized_corpus' æ¬„ä½ï¼")

        df.dropna(subset=['tokenized_corpus'], inplace=True)
        df['id'] = range(1, len(df) + 1)  # è‡ªå‹•ç”Ÿæˆå”¯ä¸€ ID
        self.stdout.write(f"âœ… æˆåŠŸè®€å– {len(df)} ç­†è³‡æ–™")
        return df

    def generate_embeddings(self, texts, model_name):
        """ä½¿ç”¨ Sentence-BERT ç”Ÿæˆèªç¾©å‘é‡"""
        self.stdout.write("ğŸš€ æ­£åœ¨ç”Ÿæˆèªç¾©å‘é‡...")
        model = SentenceTransformer(model_name)
        embeddings = model.encode(
            texts,
            batch_size=16,
            show_progress_bar=True
        )
        embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)  # æ­£è¦åŒ–
        self.stdout.write(f"âœ… èªç¾©å‘é‡ç”Ÿæˆå®Œæˆï¼Œå…± {len(embeddings)} ç­†")
        return embeddings

    def create_faiss_index(self, embeddings, ids, index_path):
        """å»ºç«‹ FAISS èªç¾©å‘é‡ç´¢å¼•ä¸¦ä¿å­˜"""
        dimension = embeddings.shape[1]
        index = faiss.IndexFlatIP(dimension)
        index = faiss.IndexIDMap(index)
        index.add_with_ids(embeddings.astype('float32'), ids)

        faiss.write_index(index, index_path)
        self.stdout.write(f"ğŸ’¾ å‘é‡ç´¢å¼•å·²ä¿å­˜è‡³ï¼š{index_path}")

    def save_ids_mapping(self, df, ids_path):
        """å°‡ ID å°æ˜ è¡¨ä¿å­˜ç‚º pkl"""
        ids_dict = {
            row['id']: {
                'title': row['movie_title'],
                'tokens': row['tokenized_corpus']
            }
            for _, row in df.iterrows()
        }
        with open(ids_path, 'wb') as f:
            pickle.dump(ids_dict, f)
        self.stdout.write(f"ğŸ’¾ ID å°æ˜ è¡¨å·²ä¿å­˜è‡³ï¼š{ids_path}")

    def handle(self, *args, **kwargs):
        try:
            # è®€å–å·²æ–·è©è³‡æ–™
            df = self.read_tokenized_excel(self.EXCEL_FILE)

            # ç”Ÿæˆèªç¾©å‘é‡
            embeddings = self.generate_embeddings(
                texts=df['tokenized_corpus'].tolist(),
                model_name=self.model_name
            )

            # å»ºç«‹ FAISS å‘é‡ç´¢å¼•
            self.create_faiss_index(
                embeddings=embeddings,
                ids=df['id'].values.astype('int64'),
                index_path=self.VECTOR_INDEX_PATH
            )

            # ä¿å­˜ ID å°æ˜ è¡¨
            self.save_ids_mapping(df, self.IDS_PATH)

            self.stdout.write(self.style.SUCCESS("ğŸ‰ èªç¾©å‘é‡ç´¢å¼•å»ºç«‹å®Œæˆï¼"))

        except Exception as e:
            self.stderr.write(f"âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
